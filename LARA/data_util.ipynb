{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from evaluation.ipynb\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import Ipynb_importer\n",
    "import evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=np.array(pd.read_csv('Data/train_data.csv',usecols=['userId','itemId','gerne']))\n",
    "test_pd=pd.read_csv('Data/test_data.csv',usecols=['itemId','gerne'])\n",
    "train_ui_data=np.array(pd.read_csv('generate_data/train_UIMatrix.csv'))\n",
    "user_attr_data=np.array(pd.read_csv('generate_data/user_attribute.csv'))\n",
    "user_attr_normal_data=np.array(pd.read_csv('generate_data/user_attribute_normal.csv'))\n",
    "\n",
    "test_pd.drop_duplicates( keep='first', inplace=True)\n",
    "test_data=np.array(test_pd)\n",
    "test_ui_data=np.array(pd.read_csv('generate_data/test_UIMatrix.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gen_train_data(train_data):\n",
    "    for train in train_data:\n",
    "        train[2]=train[2][1:-1]\n",
    "    for train in train_data:\n",
    "        gerne_list=np.int32(train[2].split(','))\n",
    "        tmp=np.linspace (0,34,18)\n",
    "        for g in gerne_list:\n",
    "            tmp[g]=tmp[g]+1\n",
    "        train[2]=np.array(tmp,dtype=np.int32)\n",
    "    pd.DataFrame(train_data).to_csv('generate_data/train_data.csv',header=None,index=False)\n",
    "     \n",
    "def gen_test_data(test_data):\n",
    "    itemId=[test[0] for test in test_data]\n",
    "    for test in test_data:\n",
    "        test[1]=test[1][1:-1]\n",
    "    attr=[]\n",
    "    for test in test_data:\n",
    "        gerne_list=np.int32(test[1].split(','))\n",
    "        tmp=np.linspace (0,34,18)\n",
    "        for g in gerne_list:\n",
    "            tmp[g]=tmp[g]+1\n",
    "        attr.append(tmp)\n",
    "    pd.DataFrame(itemId).to_csv( 'generate_data/test_item.csv',header=None,index =False)\n",
    "    pd.DataFrame(attr).to_csv( 'generate_data/test_attribute.csv',header=None,index =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gen_train_data(train_data)\\ngen_test_data(test_data)'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''gen_train_data(train_data)\n",
    "gen_test_data(test_data)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=np.array(pd.read_csv('generate_data/train_data.csv',header =None))\n",
    "neg_data=np.array(pd.read_csv('generate_data/neg_data.csv'))\n",
    "test_item=np.array(pd.read_csv('generate_data/test_item.csv').astype(np.int32))\n",
    "test_attribute=np.array(pd.read_csv('generate_data/test_attribute.csv').astype(np.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_train():\n",
    "    np.random.shuffle(train)\n",
    "    \n",
    "def shuffle_neg():\n",
    "    np.random.shuffle(neg_data)\n",
    "    \n",
    "def get_train_data(start,end):\n",
    "    data=train[start:end]\n",
    "    user=[x[0] for x in data]\n",
    "    item=[x[1] for x in data]\n",
    "    attr=[x[2][1:-1].split() for x in data]\n",
    "    user_attr_normal=user_attr_normal_data[user]\n",
    "    return user,item,attr,user_attr_normal\n",
    "\n",
    "def get_test_data():\n",
    "    return test_item,test_attribute\n",
    "\n",
    "def get_neg_data(start,end):\n",
    "    data=neg_data[start:end]\n",
    "    user=[x[0] for x in data]\n",
    "    item=[x[1] for x in data]\n",
    "    attr=[x[2][1:-1].split() for x in data]\n",
    "    user_attr_normal=user_attr_normal_data[user]\n",
    "    return user,item,attr,user_attr_normal\n",
    "\n",
    "def get_intersection_similar_user(G_user,k):\n",
    "    user_attr_matrixT=np.transpose(user_attr_data)    \n",
    "    A=np.matmul(G_user,user_attr_matrixT)   \n",
    "    intersection_rank_matrix=np.argsort(-A)\n",
    "    return intersection_rank_matrix[:,0:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_item_batch, test_G_user):\n",
    "    \n",
    "    k_value = 20\n",
    "    test_BATCH_SIZE = np.size(test_item_batch)\n",
    "    \n",
    "    test_intersection_similar_user = get_intersection_similar_user(test_G_user, k_value)\n",
    "   \n",
    "    count = 0\n",
    "    for test_i, test_userlist in zip(test_item_batch, test_intersection_similar_user):       \n",
    "        for test_u in test_userlist:\n",
    "            \n",
    "            if test_ui_data[test_u, test_i] == 1:\n",
    "                count = count + 1            \n",
    "    p_at_20 = round(count/(test_BATCH_SIZE * k_value), 4)\n",
    "           \n",
    "    ans = 0.0\n",
    "    RS = []\n",
    "    for test_i, test_userlist in zip(test_item_batch, test_intersection_similar_user):  \n",
    "        r=[]\n",
    "        for user in test_userlist:\n",
    "            r.append(test_ui_data[user][test_i])\n",
    "        RS.append(r)\n",
    "    M_at_20 = evaluation.mean_average_precision(RS)\n",
    "  \n",
    "    ans = 0.0\n",
    "    for test_i, test_userlist in zip(test_item_batch, test_intersection_similar_user):  \n",
    "        r=[]\n",
    "        for user in test_userlist:\n",
    "            r.append(test_ui_data[user][test_i])\n",
    "        ans = ans + evaluation.get_nDCG(r, k_value, method=1)\n",
    "    G_at_20 = ans/test_BATCH_SIZE\n",
    "    k_value = 10 \n",
    "    \n",
    "    count = 0\n",
    "    for test_i, test_userlist in zip(test_item_batch, test_intersection_similar_user):       \n",
    "        for test_u in test_userlist[:k_value]:\n",
    "            \n",
    "            if test_ui_data[test_u, test_i] == 1:\n",
    "                count = count + 1            \n",
    "    p_at_10 = round(count/(test_BATCH_SIZE * k_value), 4)\n",
    "         \n",
    "    ans = 0.0\n",
    "    RS = []\n",
    "    for test_i, test_userlist in zip(test_item_batch, test_intersection_similar_user):  \n",
    "        r=[]\n",
    "        for user in test_userlist[:k_value]:\n",
    "            r.append(test_ui_data[user][test_i])\n",
    "        RS.append(r)\n",
    "    M_at_10 = evaluation.mean_average_precision(RS)\n",
    "    \n",
    "\n",
    "    ans = 0.0\n",
    "    for test_i, test_userlist in zip(test_item_batch, test_intersection_similar_user):  \n",
    "        r=[]\n",
    "        for user in test_userlist[:k_value]:\n",
    "            r.append(test_ui_data[user][test_i])\n",
    "        ans = ans + evaluation.get_nDCG(r, k_value, method=1)\n",
    "    G_at_10 = ans/test_BATCH_SIZE\n",
    "    return p_at_10,p_at_20,M_at_10,M_at_20,G_at_10,G_at_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
